#!/usr/bin/env python3
"""
Path: infrastructure/source/api/init_logs.py
Version: 1
"""

import os
import sys
from pathlib import Path
from typing import Dict, List
import json

def create_log_directory_structure(base_dir: str = "logs") -> Dict[str, str]:
    """
    Create complete log directory structure
    
    Args:
        base_dir: Base directory for logs
        
    Returns:
        Dictionary mapping log types to their file paths
    """
    print(f"üóÇÔ∏è  Creating log directory structure in: {base_dir}")
    
    base_path = Path(base_dir)
    
    # Define log structure
    log_structure = {
        # Main application logs
        "main": {
            "dir": base_path,
            "file": "opendocseal.log",
            "description": "Main application logs"
        },
        
        # Access logs (HTTP requests)
        "access": {
            "dir": base_path / "access",
            "file": "access.log",
            "description": "HTTP access logs"
        },
        
        # Error logs (errors and exceptions)
        "error": {
            "dir": base_path / "error",
            "file": "error.log", 
            "description": "Error and exception logs"
        },
        
        # Audit logs (user actions, security events)
        "audit": {
            "dir": base_path / "audit",
            "file": "audit.log",
            "description": "Audit trail logs"
        },
        
        # Security logs (authentication, authorization, threats)
        "security": {
            "dir": base_path / "security", 
            "file": "security.log",
            "description": "Security event logs"
        },
        
        # Performance logs (timing, metrics)
        "performance": {
            "dir": base_path / "performance",
            "file": "performance.log",
            "description": "Performance metrics logs"
        }
    }
    
    created_dirs = []
    log_files = {}
    
    # Create directories and prepare file paths
    for log_type, config in log_structure.items():
        directory = config["dir"]
        
        try:
            directory.mkdir(parents=True, exist_ok=True)
            created_dirs.append(str(directory))
            
            file_path = directory / config["file"]
            log_files[log_type] = str(file_path.resolve())
            
            print(f"  ‚úÖ {log_type:12} -> {file_path} ({config['description']})")
            
        except Exception as e:
            print(f"  ‚ùå Failed to create {directory}: {e}")
            sys.exit(1)
    
    print(f"‚úÖ Created {len(created_dirs)} log directories")
    return log_files


def create_log_config_file(log_files: Dict[str, str], config_file: str = ".env.logs"):
    """
    Create environment configuration file for logs
    
    Args:
        log_files: Dictionary of log file paths
        config_file: Configuration file to create
    """
    print(f"üìù Creating log configuration file: {config_file}")
    
    config_content = [
        "# OpenDocSeal API - Log Configuration",
        "# Generated by init_logs.py",
        "",
        "# Main log file",
        f"LOG_FILE={log_files['main']}",
        "",
        "# Specialized log files", 
        f"LOG_ACCESS_FILE={log_files['access']}",
        f"LOG_ERROR_FILE={log_files['error']}",
        f"LOG_AUDIT_FILE={log_files['audit']}",
        f"LOG_SECURITY_FILE={log_files['security']}",
        f"LOG_PERFORMANCE_FILE={log_files['performance']}",
        "",
        "# Log configuration",
        "LOG_LEVEL=INFO",
        "LOG_FORMAT=json",
        "LOG_CORRELATION=true",
        "LOG_CONSOLE_ENABLED=true",
        "LOG_MAX_SIZE=10485760",  # 10MB
        "LOG_BACKUP_COUNT=5",
        ""
    ]
    
    try:
        with open(config_file, 'w') as f:
            f.write('\n'.join(config_content))
        
        print(f"‚úÖ Configuration file created: {config_file}")
        
    except Exception as e:
        print(f"‚ùå Failed to create config file: {e}")


def create_logrotate_config(log_files: Dict[str, str], config_file: str = "logrotate.conf"):
    """
    Create logrotate configuration for log files
    
    Args:
        log_files: Dictionary of log file paths
        config_file: Logrotate config file to create
    """
    print(f"üîÑ Creating logrotate configuration: {config_file}")
    
    config_content = [
        "# OpenDocSeal API - Logrotate Configuration",
        "# Place this file in /etc/logrotate.d/ for automatic log rotation",
        "",
    ]
    
    # Add configuration for each log file
    for log_type, log_path in log_files.items():
        config_content.extend([
            f"# {log_type.title()} logs",
            f"{log_path} {{",
            "    daily",
            "    missingok",
            "    rotate 30",
            "    compress",
            "    delaycompress", 
            "    notifempty",
            "    copytruncate",
            "    create 644 www-data www-data",
            "}",
            ""
        ])
    
    try:
        with open(config_file, 'w') as f:
            f.write('\n'.join(config_content))
        
        print(f"‚úÖ Logrotate configuration created: {config_file}")
        
    except Exception as e:
        print(f"‚ùå Failed to create logrotate config: {e}")


def set_log_permissions(log_files: Dict[str, str], docker_mode: bool = False) -> bool:
    """
    Set appropriate permissions for log directories and files
    
    Args:
        log_files: Dictionary of log file paths
        docker_mode: Optimize for Docker environment
        
    Returns:
        True if permissions were set successfully
    """
    print(f"üîê Setting log permissions (Docker mode: {docker_mode})")
    
    try:
        import stat
        
        # Permissions for directories and files
        if docker_mode:
            # More permissive for Docker
            dir_perms = 0o755
            file_perms = 0o644
        else:
            # Standard permissions
            dir_perms = 0o755
            file_perms = 0o644
        
        # Set directory permissions
        directories = set()
        for log_path in log_files.values():
            directories.add(Path(log_path).parent)
        
        for directory in directories:
            if directory.exists():
                os.chmod(directory, dir_perms)
                print(f"  ‚úÖ Directory permissions set: {directory}")
        
        # Set file permissions (create empty files if they don't exist)
        for log_type, log_path in log_files.items():
            log_file = Path(log_path)
            
            # Create empty file if it doesn't exist
            if not log_file.exists():
                log_file.touch()
            
            os.chmod(log_file, file_perms)
            print(f"  ‚úÖ File permissions set: {log_file.name}")
        
        print("‚úÖ All permissions set successfully")
        return True
        
    except Exception as e:
        print(f"‚ùå Failed to set permissions: {e}")
        return False


def check_integration() -> bool:
    """
    Check integration between log system and main application
    
    Returns:
        True if integration is working correctly
    """
    print("üîó Checking log system integration...")
    
    try:
        # Check if config can load log settings
        sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
        
        try:
            from config import get_settings
            settings = get_settings()
            
            # Check log configuration
            if hasattr(settings, 'log_files_map'):
                log_files = settings.log_files_map
                print(f"  ‚úÖ Configuration loads {len(log_files)} log files")
                
                # Check each log file path
                for log_type, log_path in log_files.items():
                    log_file_path = Path(log_path)
                    if log_file_path.parent.exists():
                        print(f"  ‚úÖ {log_type:12} -> {log_path}")
                    else:
                        print(f"  ‚ùå {log_type:12} -> {log_path} (directory missing)")
                        return False
            else:
                print("  ‚ùå Configuration missing log_files_map")
                return False
                
        except ImportError as e:
            print(f"  ‚ùå Cannot import config: {e}")
            return False
        
        # Check if logging utils can be imported
        try:
            from utils.logging import setup_logging
            print("  ‚úÖ Logging utilities available")
        except ImportError as e:
            print(f"  ‚ùå Cannot import logging utils: {e}")
            return False
        
        print("‚úÖ Integration check passed")
        return True
        
    except Exception as e:
        print(f"‚ùå Integration check failed: {e}")
        return False


def create_docker_log_config(log_files: Dict[str, str], config_file: str = "docker-logging.yml"):
    """
    Create Docker-specific logging configuration
    
    Args:
        log_files: Dictionary of log file paths
        config_file: Docker logging config file to create
    """
    print(f"üê≥ Creating Docker logging configuration: {config_file}")
    
    config_content = f"""# OpenDocSeal API - Docker Logging Configuration
version: '3.8'

services:
  opendocseal-api:
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service=opendocseal-api"
    
    # Volume mounts for log persistence
    volumes:
      - ./logs:/app/logs
      - ./logs/access:/app/logs/access
      - ./logs/error:/app/logs/error
      - ./logs/audit:/app/logs/audit
      - ./logs/security:/app/logs/security
      - ./logs/performance:/app/logs/performance
    
    # Environment variables for logging
    environment:
      - LOG_LEVEL=INFO
      - LOG_FORMAT=json
      - LOG_DIRECTORY=/app/logs
      - LOG_CONSOLE_ENABLED=true
      - LOG_FILE=/app/logs/opendocseal.log
"""
    
    for log_type, log_path in log_files.items():
        if log_type != 'main':
            env_var = f"LOG_{log_type.upper()}_FILE"
            config_content += f"      - {env_var}=/app/{log_path}\\n"
    
    try:
        with open(config_file, 'w') as f:
            f.write(config_content)
        
        print(f"‚úÖ Docker configuration created: {config_file}")
        
    except Exception as e:
        print(f"‚ùå Failed to create Docker config: {e}")


def create_log_monitoring_script(log_files: Dict[str, str], script_file: str = "monitor_logs.py"):
    """
    Create enhanced log monitoring script
    
    Args:
        log_files: Dictionary of log file paths
        script_file: Monitoring script to create
    """
    print(f"üìä Creating enhanced log monitoring script: {script_file}")
    
    script_content = '''#!/usr/bin/env python3
"""
Enhanced log monitoring script for OpenDocSeal API
Monitors log files for errors, performance issues, and security events
"""

import os
import time
import json
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Any
import re
import argparse

# Log files to monitor
LOG_FILES = ''' + json.dumps(log_files, indent=4) + '''

class EnhancedLogMonitor:
    """Enhanced log file monitor with alerting and analysis"""
    
    def __init__(self):
        self.file_positions = {}
        self.alert_thresholds = {
            'error_rate': 10,  # errors per minute
            'response_time': 5.0,  # seconds
            'disk_usage': 85,  # percentage
        }
        
    def tail_file(self, filepath: str, num_lines: int = 10) -> List[str]:
        """Get last N lines from file"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                return lines[-num_lines:] if lines else []
        except (FileNotFoundError, PermissionError):
            return []
    
    def check_file_size(self, filepath: str) -> int:
        """Get file size in bytes"""
        try:
            return os.path.getsize(filepath)
        except OSError:
            return 0
    
    def analyze_log_patterns(self, log_type: str, content: str) -> Dict[str, Any]:
        """Analyze log content for patterns and issues"""
        analysis = {
            'error_count': 0,
            'warning_count': 0,
            'performance_issues': [],
            'security_events': [],
            'unique_errors': set(),
            'response_times': [],
        }
        
        lines = content.split('\\n')
        
        for line in lines:
            if not line.strip():
                continue
                
            try:
                # Try to parse as JSON log
                if line.strip().startswith('{'):
                    log_entry = json.loads(line.strip())
                    
                    # Count errors and warnings
                    level = log_entry.get('level', '')
                    if level == 'ERROR':
                        analysis['error_count'] += 1
                        if 'message' in log_entry:
                            analysis['unique_errors'].add(log_entry['message'])
                    elif level == 'WARNING':
                        analysis['warning_count'] += 1
                    
                    # Check for performance issues
                    if 'duration_seconds' in log_entry:
                        duration = float(log_entry['duration_seconds'])
                        analysis['response_times'].append(duration)
                        
                        if duration > self.alert_thresholds['response_time']:
                            analysis['performance_issues'].append({
                                'timestamp': log_entry.get('timestamp'),
                                'operation': log_entry.get('operation'),
                                'duration': duration
                            })
                    
                    # Check for security events
                    if log_entry.get('event_type') == 'security':
                        analysis['security_events'].append({
                            'timestamp': log_entry.get('timestamp'),
                            'event': log_entry.get('security_event'),
                            'severity': log_entry.get('severity'),
                            'ip_address': log_entry.get('ip_address')
                        })
                
                else:
                    # Parse text logs
                    if 'ERROR' in line:
                        analysis['error_count'] += 1
                    elif 'WARNING' in line:
                        analysis['warning_count'] += 1
                        
            except (json.JSONDecodeError, ValueError):
                # Skip unparseable lines
                continue
        
        # Convert set to list for JSON serialization
        analysis['unique_errors'] = list(analysis['unique_errors'])
        
        return analysis
    
    def check_recent_logs(self, log_type: str, minutes: int = 30) -> Dict[str, Any]:
        """Check recent log entries for issues"""
        log_path = LOG_FILES.get(log_type)
        if not log_path or not os.path.exists(log_path):
            return {'error': f'Log file not found: {log_path}'}
        
        cutoff_time = datetime.now() - timedelta(minutes=minutes)
        recent_content = []
        
        try:
            with open(log_path, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        # Try to extract timestamp
                        if line.strip().startswith('{'):
                            log_entry = json.loads(line.strip())
                            timestamp_str = log_entry.get('timestamp', '')
                            if timestamp_str:
                                log_time = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
                                if log_time >= cutoff_time:
                                    recent_content.append(line)
                        else:
                            # For text logs, include all recent lines (simplified)
                            recent_content.append(line)
                    except (json.JSONDecodeError, ValueError):
                        continue
        
        except Exception as e:
            return {'error': str(e)}
        
        # Analyze recent content
        content_str = ''.join(recent_content)
        analysis = self.analyze_log_patterns(log_type, content_str)
        analysis['recent_lines'] = len(recent_content)
        
        return analysis
    
    def generate_detailed_report(self) -> Dict[str, Any]:
        """Generate comprehensive monitoring report"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'log_files': {},
            'system_health': {},
            'alerts': [],
            'summary': {}
        }
        
        total_size = 0
        total_errors = 0
        total_warnings = 0
        performance_issues = 0
        security_events = 0
        
        # Analyze each log file
        for log_type, log_path in LOG_FILES.items():
            file_analysis = {
                'path': log_path,
                'exists': os.path.exists(log_path),
                'size_bytes': 0,
                'size_mb': 0,
                'analysis': {}
            }
            
            if file_analysis['exists']:
                size = self.check_file_size(log_path)
                file_analysis['size_bytes'] = size
                file_analysis['size_mb'] = round(size / (1024 * 1024), 2)
                total_size += size
                
                # Analyze recent entries
                analysis = self.check_recent_logs(log_type, 60)  # Last hour
                if 'error' not in analysis:
                    file_analysis['analysis'] = analysis
                    total_errors += analysis.get('error_count', 0)
                    total_warnings += analysis.get('warning_count', 0)
                    performance_issues += len(analysis.get('performance_issues', []))
                    security_events += len(analysis.get('security_events', []))
                    
                    # Generate alerts
                    if analysis.get('error_count', 0) > self.alert_thresholds['error_rate']:
                        report['alerts'].append({
                            'type': 'high_error_rate',
                            'severity': 'high',
                            'message': f'{log_type} has {analysis["error_count"]} errors in the last hour',
                            'log_type': log_type
                        })
                    
                    if analysis.get('performance_issues'):
                        report['alerts'].append({
                            'type': 'performance_degradation', 
                            'severity': 'medium',
                            'message': f'{len(analysis["performance_issues"])} slow operations detected',
                            'log_type': log_type
                        })
                    
                    if analysis.get('security_events'):
                        report['alerts'].append({
                            'type': 'security_events',
                            'severity': 'high',
                            'message': f'{len(analysis["security_events"])} security events detected',
                            'log_type': log_type
                        })
            
            report['log_files'][log_type] = file_analysis
        
        # System health metrics
        report['system_health'] = {
            'total_log_size_mb': round(total_size / (1024 * 1024), 2),
            'disk_usage_check': self.check_disk_usage(),
            'oldest_log_age': self.get_oldest_log_age(),
        }
        
        # Summary
        report['summary'] = {
            'total_errors': total_errors,
            'total_warnings': total_warnings,
            'performance_issues': performance_issues,
            'security_events': security_events,
            'alerts_count': len(report['alerts']),
            'health_status': 'healthy' if len(report['alerts']) == 0 else 'issues_detected'
        }
        
        return report
    
    def check_disk_usage(self) -> Dict[str, Any]:
        """Check disk usage for log directory"""
        try:
            log_dir = Path(list(LOG_FILES.values())[0]).parent
            stat = os.statvfs(log_dir)
            
            total_space = stat.f_frsize * stat.f_blocks
            free_space = stat.f_frsize * stat.f_available
            used_space = total_space - free_space
            usage_percent = (used_space / total_space) * 100
            
            return {
                'total_gb': round(total_space / (1024**3), 2),
                'used_gb': round(used_space / (1024**3), 2),
                'free_gb': round(free_space / (1024**3), 2),
                'usage_percent': round(usage_percent, 1),
                'alert': usage_percent > self.alert_thresholds['disk_usage']
            }
        except Exception as e:
            return {'error': str(e)}
    
    def get_oldest_log_age(self) -> Dict[str, Any]:
        """Get age of oldest log files"""
        oldest_age = None
        oldest_file = None
        
        for log_type, log_path in LOG_FILES.items():
            if os.path.exists(log_path):
                mtime = os.path.getmtime(log_path)
                age = time.time() - mtime
                
                if oldest_age is None or age > oldest_age:
                    oldest_age = age
                    oldest_file = log_type
        
        if oldest_age is not None:
            return {
                'oldest_file': oldest_file,
                'age_hours': round(oldest_age / 3600, 1),
                'age_days': round(oldest_age / 86400, 1)
            }
        
        return {'error': 'No log files found'}
    
    def print_detailed_report(self):
        """Print comprehensive monitoring report"""
        report = self.generate_detailed_report()
        
        print("üìä OpenDocSeal API - Enhanced Log Monitoring Report")
        print("=" * 60)
        print(f"üïê Timestamp: {report['timestamp']}")
        
        # Health status
        status = report['summary']['health_status']
        status_icon = "‚úÖ Healthy" if status == "healthy" else "‚ö†Ô∏è Issues Detected"
        print(f"üìä Status: {status_icon}")
        
        # Alerts
        if report['alerts']:
            print(f"\\nüö® Active Alerts ({len(report['alerts'])}):")
            for alert in report['alerts']:
                severity_icon = {"high": "üî¥", "medium": "üü°", "low": "üü¢"}.get(alert['severity'], "‚ÑπÔ∏è")
                print(f"  {severity_icon} {alert['type']}: {alert['message']}")
        else:
            print("\\n‚úÖ No active alerts")
        
        # Log files summary
        print("\\nüìÅ Log Files:")
        for log_type, info in report['log_files'].items():
            if info['exists']:
                status = "‚úÖ"
                size_info = f"{info['size_mb']}MB"
                analysis = info.get('analysis', {})
                error_count = analysis.get('error_count', 0)
                warning_count = analysis.get('warning_count', 0)
                
                details = f" (E:{error_count} W:{warning_count})" if error_count or warning_count else ""
                print(f"  {status} {log_type:12} - {size_info:8} - {info['path']}{details}")
            else:
                print(f"  ‚ùå {log_type:12} - Missing  - {info['path']}")
        
        # System health
        print("\\nüñ•Ô∏è System Health:")
        disk_info = report['system_health']['disk_usage_check']
        if 'error' not in disk_info:
            disk_status = "‚ö†Ô∏è" if disk_info.get('alert', False) else "‚úÖ"
            print(f"  {disk_status} Disk Usage: {disk_info['usage_percent']}% ({disk_info['used_gb']}GB / {disk_info['total_gb']}GB)")
        
        age_info = report['system_health']['oldest_log_age']
        if 'error' not in age_info:
            print(f"  üìÖ Oldest Log: {age_info['oldest_file']} ({age_info['age_hours']}h old)")
        
        # Summary
        print(f"\\nüìà Summary:")
        summary = report['summary']
        print(f"  Total Errors: {summary['total_errors']}")
        print(f"  Total Warnings: {summary['total_warnings']}")
        print(f"  Performance Issues: {summary['performance_issues']}")
        print(f"  Security Events: {summary['security_events']}")
        print(f"  Log Size: {report['system_health']['total_log_size_mb']}MB")

def main():
    """Main monitoring function with enhanced options"""
    parser = argparse.ArgumentParser(description="Enhanced OpenDocSeal API log monitor")
    parser.add_argument("--json", action="store_true", help="JSON output for programmatic use")
    parser.add_argument("--alerts", action="store_true", help="Show only alerts")
    parser.add_argument("--log-type", help="Monitor specific log type only")
    parser.add_argument("--minutes", type=int, default=60, help="Time window in minutes (default: 60)")
    parser.add_argument("--watch", action="store_true", help="Continuous monitoring mode")
    parser.add_argument("--interval", type=int, default=30, help="Watch interval in seconds (default: 30)")
    
    args = parser.parse_args()
    
    monitor = EnhancedLogMonitor()
    
    def run_monitor():
        if args.json:
            # JSON output for programmatic use
            report = monitor.generate_detailed_report()
            print(json.dumps(report, indent=2, default=str))
        elif args.alerts:
            # Show only alerts
            report = monitor.generate_detailed_report()
            if report['alerts']:
                print("üö® Active Alerts:")
                for alert in report['alerts']:
                    print(f"  ‚Ä¢ {alert['type']}: {alert['message']}")
            else:
                print("‚úÖ No active alerts")
        else:
            # Human-readable detailed output
            monitor.print_detailed_report()
    
    if args.watch:
        # Continuous monitoring
        print(f"üëÄ Watching logs (interval: {args.interval}s, Ctrl+C to stop)")
        try:
            while True:
                os.system('clear' if os.name == 'posix' else 'cls')  # Clear screen
                run_monitor()
                time.sleep(args.interval)
        except KeyboardInterrupt:
            print("\\nüëã Monitoring stopped")
    else:
        # Single run
        run_monitor()

if __name__ == "__main__":
    main()
'''
    
    try:
        with open(script_file, 'w') as f:
            f.write(script_content)
        
        # Make script executable
        os.chmod(script_file, 0o755)
        
        print(f"‚úÖ Enhanced monitoring script created: {script_file}")
        print(f"   Usage: python {script_file}")
        print(f"   Usage: python {script_file} --json")
        print(f"   Usage: python {script_file} --watch")
        print(f"   Usage: python {script_file} --alerts")
        
    except Exception as e:
        print(f"‚ùå Failed to create monitoring script: {e}")


def create_systemd_service_file(log_files: Dict[str, str], service_file: str = "opendocseal-api.service"):
    """
    Create systemd service file for production deployment
    
    Args:
        log_files: Dictionary of log file paths  
        service_file: Service file to create
    """
    print(f"‚öôÔ∏è Creating systemd service file: {service_file}")
    
    service_content = f"""[Unit]
Description=OpenDocSeal API Service
After=network.target mongodb.service

[Service]
Type=exec
User=www-data
Group=www-data
WorkingDirectory=/opt/opendocseal/api
Environment=PYTHONPATH=/opt/opendocseal/api
Environment=LOG_LEVEL=WARNING
Environment=LOG_FORMAT=json
Environment=LOG_CONSOLE_ENABLED=false
ExecStart=/opt/opendocseal/api/venv/bin/python -m uvicorn main:app --host 0.0.0.0 --port 8000 --workers 4
ExecReload=/bin/kill -s HUP $MAINPID
KillMode=mixed
TimeoutStopSec=5
PrivateTmp=true
Restart=always
RestartSec=10

# Logging
StandardOutput=append:/opt/opendocseal/api/logs/systemd-stdout.log
StandardError=append:/opt/opendocseal/api/logs/systemd-stderr.log

# Security
NoNewPrivileges=true
ProtectSystem=strict
ProtectHome=true
ReadWritePaths=/opt/opendocseal/api/logs
PrivateDevices=true
ProtectKernelTunables=true
ProtectKernelModules=true
ProtectControlGroups=true

[Install]
WantedBy=multi-user.target
"""
    
    try:
        with open(service_file, 'w') as f:
            f.write(service_content)
        
        print(f"‚úÖ Systemd service file created: {service_file}")
        print(f"   Install with:")
        print(f"     sudo cp {service_file} /etc/systemd/system/")
        print(f"     sudo systemctl daemon-reload")
        print(f"     sudo systemctl enable opendocseal-api")
        print(f"     sudo systemctl start opendocseal-api")
        
    except Exception as e:
        print(f"‚ùå Failed to create service file: {e}")
    """
    Create log monitoring script
    
    Args:
        log_files: Dictionary of log file paths
        script_file: Monitoring script to create
    """
    print(f"üìä Creating log monitoring script: {script_file}")
    
    script_content = '''#!/usr/bin/env python3
"""
Log monitoring script for OpenDocSeal API
Monitors log files for errors, performance issues, and security events
"""

import os
import time
import json
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Any

# Log files to monitor
LOG_FILES = ''' + json.dumps(log_files, indent=4) + '''

class LogMonitor:
    """Simple log file monitor"""
    
    def __init__(self):
        self.file_positions = {}
        
    def tail_file(self, filepath: str, num_lines: int = 10) -> List[str]:
        """Get last N lines from file"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                return lines[-num_lines:] if lines else []
        except (FileNotFoundError, PermissionError):
            return []
    
    def check_file_size(self, filepath: str) -> int:
        """Get file size in bytes"""
        try:
            return os.path.getsize(filepath)
        except OSError:
            return 0
    
    def check_recent_errors(self, log_type: str, minutes: int = 30) -> List[Dict]:
        """Check for recent errors in logs"""
        log_path = LOG_FILES.get(log_type)
        if not log_path or not os.path.exists(log_path):
            return []
        
        errors = []
        cutoff_time = datetime.now() - timedelta(minutes=minutes)
        
        try:
            with open(log_path, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        # Try to parse as JSON log
                        log_entry = json.loads(line.strip())
                        timestamp_str = log_entry.get('timestamp', '')
                        
                        if timestamp_str:
                            log_time = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
                            if log_time >= cutoff_time and log_entry.get('level') in ['ERROR', 'CRITICAL']:
                                errors.append(log_entry)
                                
                    except (json.JSONDecodeError, ValueError):
                        # Skip non-JSON lines
                        continue
                        
        except Exception as e:
            print(f"Error reading {log_path}: {e}")
        
        return errors
    
    def generate_report(self) -> Dict[str, Any]:
        """Generate monitoring report"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'log_files': {},
            'recent_errors': {},
            'summary': {}
        }
        
        total_size = 0
        total_errors = 0
        
        for log_type, log_path in LOG_FILES.items():
            if os.path.exists(log_path):
                size = self.check_file_size(log_path)
                recent_lines = self.tail_file(log_path, 5)
                recent_errors = self.check_recent_errors(log_type, 60)  # Last hour
                
                report['log_files'][log_type] = {
                    'path': log_path,
                    'size_bytes': size,
                    'size_mb': round(size / (1024 * 1024), 2),
                    'exists': True,
                    'recent_lines_count': len(recent_lines)
                }
                
                report['recent_errors'][log_type] = len(recent_errors)
                
                total_size += size
                total_errors += len(recent_errors)
                
            else:
                report['log_files'][log_type] = {
                    'path': log_path,
                    'exists': False,
                    'size_bytes': 0,
                    'size_mb': 0
                }
        
        report['summary'] = {
            'total_log_size_mb': round(total_size / (1024 * 1024), 2),
            'total_recent_errors': total_errors,
            'monitoring_healthy': total_errors < 10  # Threshold for "healthy"
        }
        
        return report
    
    def print_report(self):
        """Print monitoring report"""
        report = self.generate_report()
        
        print("üìä OpenDocSeal API - Log Monitoring Report")
        print("=" * 50)
        print(f"üïê Timestamp: {report['timestamp']}")
        print(f"üìä Status: {'‚úÖ Healthy' if report['summary']['monitoring_healthy'] else '‚ö†Ô∏è  Issues Detected'}")
        print()
        
        print("üìÅ Log Files:")
        for log_type, info in report['log_files'].items():
            status = "‚úÖ" if info['exists'] else "‚ùå"
            size_info = f"{info['size_mb']}MB" if info['exists'] else "N/A"
            print(f"  {status} {log_type:12} - {size_info:8} - {info['path']}")
        
        print()
        print("üö® Recent Errors (last hour):")
        total_recent = sum(report['recent_errors'].values())
        if total_recent == 0:
            print("  ‚úÖ No recent errors detected")
        else:
            for log_type, count in report['recent_errors'].items():
                if count > 0:
                    print(f"  ‚ö†Ô∏è  {log_type:12} - {count} errors")
        
        print()
        print(f"üìà Summary:")
        print(f"  Total log size: {report['summary']['total_log_size_mb']}MB")
        print(f"  Recent errors: {report['summary']['total_recent_errors']}")

def main():
    """Main monitoring function"""
    monitor = LogMonitor()
    
    if len(os.sys.argv) > 1 and os.sys.argv[1] == "--json":
        # JSON output for programmatic use
        report = monitor.generate_report()
        print(json.dumps(report, indent=2))
    else:
        # Human-readable output
        monitor.print_report()

if __name__ == "__main__":
    main()
'''
    
    try:
        with open(script_file, 'w') as f:
            f.write(script_content)
        
        # Make script executable
        os.chmod(script_file, 0o755)
        
        print(f"‚úÖ Monitoring script created: {script_file}")
        print(f"   Usage: python {script_file}")
        print(f"   Usage: python {script_file} --json")
        
    except Exception as e:
        print(f"‚ùå Failed to create monitoring script: {e}")


def validate_log_setup() -> bool:
    """
    Validate that log setup is correct
    
    Returns:
        True if validation passes, False otherwise
    """
    print("üîç Validating log setup...")
    
    # Check if we can import the configuration
    try:
        sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
        from config import get_settings
        
        settings = get_settings()
        
        # Check that all log files are accessible
        log_files = settings.log_files_map
        
        for log_type, log_path in log_files.items():
            log_file = Path(log_path)
            
            # Check directory exists
            if not log_file.parent.exists():
                print(f"‚ùå Log directory missing: {log_file.parent}")
                return False
            
            # Check we can write to the location
            try:
                test_file = log_file.parent / f".test_write_{os.getpid()}"
                test_file.write_text("test")
                test_file.unlink()
                print(f"‚úÖ {log_type:12} -> {log_path} (writable)")
            except Exception as e:
                print(f"‚ùå Cannot write to {log_path}: {e}")
                return False
        
        print("‚úÖ Log setup validation passed")
        return True
        
    except Exception as e:
        print(f"‚ùå Validation failed: {e}")
        return False


def print_usage():
    """Print usage information"""
    print("""
üîß OpenDocSeal API - Log Initialization Script

Usage:
    python init_logs.py [OPTIONS]

Options:
    --dir <path>        Base directory for logs (default: logs)
    --config            Create .env.logs configuration file
    --logrotate         Create logrotate configuration
    --monitor           Create log monitoring script
    --validate          Validate log setup
    --all               Do everything (default)
    --help              Show this help

Examples:
    python init_logs.py                    # Create logs/ directory structure
    python init_logs.py --dir /var/log/api # Use custom directory
    python init_logs.py --all              # Full setup with all files
    python init_logs.py --validate         # Validate existing setup
""")


def main():
    """Main initialization function"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Initialize OpenDocSeal API log directory structure",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument(
        "--dir", 
        default="logs", 
        help="Base directory for logs (default: logs)"
    )
    
    parser.add_argument(
        "--config", 
        action="store_true", 
        help="Create .env.logs configuration file"
    )
    
    parser.add_argument(
        "--logrotate", 
        action="store_true", 
        help="Create logrotate configuration"
    )
    
    parser.add_argument(
        "--monitor", 
        action="store_true", 
        help="Create log monitoring script"
    )
    
    parser.add_argument(
        "--validate", 
        action="store_true", 
        help="Validate log setup"
    )
    
    parser.add_argument(
        "--all", 
        action="store_true", 
        help="Do everything (create dirs, config, logrotate, monitor)"
    )
    
    parser.add_argument(
        "--permissions",
        action="store_true",
        help="Set proper permissions for log directories"
    )
    
    parser.add_argument(
        "--docker",
        action="store_true", 
        help="Optimize for Docker environment"
    )
    
    args = parser.parse_args()
    
    # If no specific action, default to --all
    if not any([args.config, args.logrotate, args.monitor, args.validate, args.permissions]):
        args.all = True
    
    print("üöÄ OpenDocSeal API - Log Initialization")
    print("=" * 60)
    
    success = True
    
    # Always create directory structure
    log_files = create_log_directory_structure(args.dir)
    
    # Set permissions if requested
    if args.all or args.permissions:
        success &= set_log_permissions(log_files, args.docker)
    
    if args.all or args.config:
        create_log_config_file(log_files)
    
    if args.all or args.logrotate:
        create_logrotate_config(log_files)
    
    if args.all or args.monitor:
        create_log_monitoring_script(log_files)
    
    if args.validate:
        success = validate_log_setup()
    
    # Final integration check
    if success and args.all:
        success &= check_integration()
    
    print()
    if success:
        print("üéâ Log initialization completed successfully!")
        print()
        print("üìã Next steps:")
        print("  1. Review generated configuration files")
        print("  2. Update your .env file with log settings")  
        print("  3. Test configuration: python init_logs.py --validate")
        print("  4. Test the API with: python run.py")
        print("  5. Monitor logs with: python monitor_logs.py")
        print()
        print("üîß Additional tools:")
        print("  ‚Ä¢ Validate imports: python validate_imports.py")
        print("  ‚Ä¢ Fix imports: python fix_imports.py --apply")
        print("  ‚Ä¢ Run tests: python tests/run_tests.py --all")
    else:
        print("‚ùå Log initialization completed with errors")
        sys.exit(1)


if __name__ == "__main__":
    main()